{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAAA3CAYAAACW0n9NAAAJ8UlEQVR4Ae1bwWvbTBb//pX5J3QUuZSyJKf4JujBbA+GwBoKxpdiFhZTWEwPRQSCCJSySzGFj3WWD3QI+FDQpTiHohyW9BDcQ8CHgA4BHT74LaMZ2ZI9Go9s2ZHLCwTbo5n33vxmfjPvvRn9BvojBAiBShD4rRIpJIQQIARAZKJJQAhUhACRqSIgSQwhQGSiOUAIVIQAkakiIEkMIUBkojlACFSEAJGpIiBJDCFAZKI5QAhUhACRqSIgSQwhQGSiOUAIVIQAkakiIEkMIUBkojlACFSEAJGpIiBJDCFAZKI5QAhUhACRqSIgSQwh8GxkmgYBpn/ucwCm8P8VINqnStJVDwT+nGFy5aLrvIDFGBiz0eh4+M9/fdw9VWfiM5ApRnjZQvdqWl0vjCRN4J6NMDOqS5V+FQTiH0N0jziBFP9/OcNfmy4mFRHKmEzx1wGcVw4aqWFHjeQ3LxPlFl687mH4Xb/2R9c92G/Hz7BDEJl+FYKY9iO+cdFgDJbTx+g2QiwbRrdDtC1OLgcf/92D/WaEKpZ2YzKlHQjPrYTlvetl0kQI3p+AsQbc76nZaSv5+ThGz2pieL9UvpefRKa9wFwXJT9HCWHsjq8kyvSzk8xj92aG0ZmF9h/b+ywlyTTF8BVndAujBwVq3wZiOy3YecKLE7B3wXyFUEjYYRGRaYfg1kz0DP4bC6w5VBKJGzu7aoExC+53IA4GsKwBgoI9wLRz5cgUjdHjvuexh1ChIWU7U8UmcYCBxdD/uqXFCr1mRUQmM5wOv1b8tQ+LNeDdFvdl+jsnUw/jxMGawLUYuv6yt1XcXvWkHJnkzmN9mChk8e1SBHnNL6seKO/gwnhF850XEZl2DnEtFEwxbDKwjq+Jy2ME7xjYq3Tnkr8LPCrTbpUiU3jJY6KC3eV+iCbftU5dhIrNJ4m1WvpAb3rdR9N5AfuoCe8mIySeYHBkoSeWEfDA0rFsDL5l6qztcTGZdqt3rWHPUGGK8bsmnJc27KaXy2bF3wawrXTFjjE5d2AdDTApA3WVPXrwk2xcUeyzourWw4kqc6coO7lc+Fdip+pv5eqVIFO687ThL8Vq0e0IvVMG67WHULlTyrbvVTuahOPWg5NkVWTdzCohdjWGNOlx97EhYjPlDrkCrywoINPO9RbZ83zl4aWDNj+aeBihxRa4AnKFnrs/d/h0KrwN9+Z57J390RZjXRSnL5k1+SASZMpUeI5QJ3k38MZNsnvbJMfMyZTGSxmDrJciLe50+hgGM01iYQKXpyjPFytBHgM+iLJzcYA+r5shitgRs0mPEN4xg/N51Z3My83+UpFpW70R7q76GH7P6qn5d46vjHnFImVhQRSBay7mTVZ6Z6cZ2Fngwb1eWqFTGJ9CfOo4aH0wOXAXsY8+yRXB7yji/vshHMYyWKQGmH+ak+nGFafHut2lUO8EA8bQuioADFOML8a4A7A6wDKDmEt6xBj/nWHwrVCh4oGKTJvqvcOo00L7bTdxbReTUaG2bkX3Y3jXCdIibrBczP0FOaGy7g+eeNJpsKhTWX8iBOdttDpdtI91c6OEQuniadPcMhxZmYtyl9a2XWOKMZnSTN1GyqShKx1YMS7C+O3SqiF3xOxOBXCCpX79ipCCAhWZ0qqb6hU77kGRad5lkZk9uVh4C/xAPUkXZ106TrCMy502r+5TuPXr58Z6jalLWDwe3BOxwI7d1RjQeI4W22FIJp63577zkp9ZLDf/ZOajrd2ZZHWFiwe5I+ZS6rzjqvR7XuvSLw2ZNtZ7uGRa9QAAEW/kg3B+HlPFRF8ajMzP6siULvhFZBI3Iqx57J0xYh4/btNXMzLJycY23u7FpGv9vibG+S5cySxxBEAtjH4uuh75XYj0O7/n10O304J3c4fx+z4GF320X+czVKKlhkyl9aa2HC6ZxE2WLHGkO53LuPL4Ir2xUgbrFB+Tz+rIlN7OUZ4XyRsRVtEuK3emcqFDvn9mZJKTjb3xN7woKgPbTFIhb4b8lWRU8kHg5P3yjQt+jiCTEY8+Bp+nSNKaxwOZ4hWDs7o6achUVu/ceD2Z4hsPrVct9MdFsaIQxFPzrVddfLrN55/Llpvq41onHziumXgJIq7NJR94fJF6AKWwngNk8GUNmZ4m8M4ctC4mmgSXUJO6eex0gPGDxDKOEF714Vgy21x0qTWZ4+JGhIHRyipGZEq3z1xgqhRXVChjknVkfPTRZQzNNEvHV5MjCxY7Sa59cOmR34NzHgpg7wMEDyKdu1iNOHFV7qiGTGX1zrupI5PMGvHsZy55Mm8sv4Tw0svDucWmbLmpPqGW7+6MpbsOML1qw7ZEPBEmr8ZE8N86i3uWJlg/RYgi/X+88tqNnkxzgrDVI5llJJHc/RSp/OXUuN0Zal+3EPGigY4VpYsCDZnuMPwbT303YMt0uEiFN/FJc01jITr/LSGk5SqvIWVrRt88dE9t2KcOnNd9jH9GCD920Tiy0XAcNN+Nl+5bhXCtTOqWr6bZDNVcuIZMnKSl9XLBOjIBM78Lm9mwj3SDFCO8cGBZDtzsQTXKlpvqSwGJMLlYwjXiaegGbP5GgNNE/3rZLddjHf0IEAT6/7vHVH/6qScT5KFteo8ubVX0Gf8Yof9avrdkvYDTcTG60XsGXFZy/DK/EVEkXV+uIZO+Yemnuzqv4NmmDEmnX5rijOohwPh/WSv1ZMrWNP+uJ5OQw3fOsplHcwtWa+5QnzHWq1YVl6whk2zI34Hb5kC1WD9/ImzY3PMS0vdHJp7ObjKZONB3rczTZHue30QXWUceRE4/u/Bzq+BzkSmE9ya9A1amZ5vW3Z0+c6zL2G5CJu7CZpMlZeQb1E3OnhYur0ELZZU9kkkcyFra+EFpo7YwPLfnd/Z4xfBjC91/9DDwl12UKskUYfLFg/eulbjAdrOHwcUQE8VVKj4B02tQ2o5U9HCX+syxNuvM9NqD974rkgMv2+hfeBir3nW7H6L1z929uhNeNmDNF2Qz21W19komvp3y90y2yeWrOmFWViWZzDQm/n5Hf7nXUJJZNR5f7FOfmVVb1prB73Thq96f21Jy0pwnLXhMW4H8PZMJwNMErtPOnRtVgcl6Gc9AJsRYzV6tt3TzGvvWt7mlZVrG+RODMk3X1OVZ5uXEz5ommsf7JxM35jHA4Ez9qobG1i0fzRBcy5T6lpKo+a+AAM+WthUZy8379jxk2txeakkI1BYBIlNth4YMOzQEiEyHNmJkb20RIDLVdmjIsENDgMh0aCNG9tYWASJTbYeGDDs0BIhMhzZiZG9tESAy1XZoyLBDQ4DIdGgjRvbWFgEiU22Hhgw7NASITIc2YmRvbREgMtV2aMiwQ0OAyHRoI0b21hYBIlNth4YMOzQEiEyHNmJkb20RIDLVdmjIsEND4P8RjC6KP1S7lgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "collect-distribution",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기\n",
    "## 목적\n",
    "인공지능이 문장을 이해하는 방식과 작문을 가르치는 법 배우기\n",
    "- 순환신경망(RNN)을 사용해 문장 만들기\n",
    "- 언어모델(Language Model) 사용\n",
    ": 일정한 단어 시퀀스가 주어진다면 다음 단어, 그 다음 단어를 계속해서 예측해 낸다.\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-examination",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드\n",
    "- 클라우드 환경에 있는 데이터 다운로드\n",
    "\n",
    " mkdir -p ~/aiffel/lyricist/models                                           \n",
    " ln -s ~/data ~/aiffel/lyricist/data\n",
    "## 2. 데이터 읽어오기\n",
    "- glob 모듈을 사용하여 모듈을 파일을 읽어온다.\n",
    "- glob 모듈을 활용하여 txt파일을 읽어온 후, raw_corpus 리스트에 문장단위로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "light-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '', \"Jesus died for somebody's sins but not mine\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/AIFFEL_PROJECT/EXPLORATION_4/lyricist/data_1/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])  #앞에서부터 4라인만 화면에 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-envelope",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제\n",
    "### 토큰화(Tokenize)\n",
    "텍스트 생성 모델에서 단어사전을 만드는 데 그 기준으로 문장을 일정한 기준으로 쪼개는 과정\n",
    "- 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "### 데이터 전처리 과정\n",
    "- 정규표현식을 이용한 corpus 생성\n",
    "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "taken-bailey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "#전처리를 위해 정규표현식\n",
    "\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certified-moscow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> jesus died for somebody s sins but not mine <end>',\n",
       " '<start> meltin in a pot of thieves <end>',\n",
       " '<start> wild card up my sleeve <end>',\n",
       " '<start> thick heart of stone <end>',\n",
       " '<start> my sins my own <end>',\n",
       " '<start> they belong to me , me <end>',\n",
       " '<start> people say beware ! <end>',\n",
       " '<start> but i don t care <end>',\n",
       " '<start> the words are just <end>',\n",
       " '<start> rules and regulations to me , me <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모음\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(preprocessed_sentence.split()) > 15: continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-passport",
   "metadata": {},
   "source": [
    "### 벡터화(vectorize)\n",
    "_tf.keras.preprocessing.text.Tokenizer_ 패키지를 사용해 정제된 데이터를 토큰화하고, 단어사전을 만들어주며 데이터를 숫자로 변환까지 해준다. 이 과정을 _벡터화(vectorize)_ 라 하며, 숫자로 변환된 데이터를 _텐서(tensor)_ 라고 칭한다.\n",
    "\n",
    "#### 텐서문서참고용 사이트\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valuable-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  631  731 ...    0    0    0]\n",
      " [   2    1   14 ...    0    0    0]\n",
      " [   2  465 1969 ...    0    0    0]\n",
      " ...\n",
      " [   2   38  905 ...    0    0    0]\n",
      " [   2   38   68 ...    0    0    0]\n",
      " [   2    8   83 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7ffac98cf890>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  #15000단어를 기억할 수 있는 tokenizer를 만든다\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\" #15000단어에 포함되지 못한 단어는 \"<unk>\" 로 변환\n",
    "    )\n",
    "# corpus를 이용해 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰주기 \n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "announced-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  631  731   28  247   17 2250   33   69  238]\n",
      " [   2    1   14    9 1968   20 2718    3    0    0]\n",
      " [   2  465 1969   29   13 2805    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])\n",
    "\n",
    "#덴서 데이터는 모두 정수로 이루어져있고, 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-sandwich",
   "metadata": {},
   "source": [
    "### tensor에서 소스문장과 타겟문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dramatic-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "#단어 사전이 어떻게 구축되었는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subject-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  631  731   28  247   17 2250   33   69  238    3    0    0    0]\n",
      "[ 631  731   28  247   17 2250   33   69  238    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-comment",
   "metadata": {},
   "source": [
    "### tf.data.Dataset객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vertical-montreal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-wildlife",
   "metadata": {},
   "source": [
    "## 4. 인공지능 학습시키기\n",
    "- tokenize() 함수로 데이터를 Tensor로 변환\n",
    "- klearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리\n",
    "- 총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "- 단어장의 크기는 12,000 이상으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "combined-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터와 평가 데이터를 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input,test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadly-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", np.shape(enc_train))\n",
    "print(\"Target Train:\", np.shape(dec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reduced-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infrared-influence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 223s 357ms/step - loss: 3.9876\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 222s 364ms/step - loss: 3.0337\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 221s 363ms/step - loss: 2.8517\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 221s 362ms/step - loss: 2.7114\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 220s 361ms/step - loss: 2.5990\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 220s 360ms/step - loss: 2.4925\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 220s 360ms/step - loss: 2.3980\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 219s 359ms/step - loss: 2.3135\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 219s 359ms/step - loss: 2.2362\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 219s 359ms/step - loss: 2.1641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffac964af10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "british-addition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-roulette",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기\n",
    "- 모델 loss 확인\n",
    "- 작문을 시켜보고 직접 평가\n",
    "- generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "finished-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 - 16s - loss: 2.0839\n",
      "test loss: 2.0838570594787598\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val, dec_val, verbose=2, batch_size=256)\n",
    "\n",
    "print('test loss:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "medical-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "looking-invention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suited-instruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate the way you lie <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "concrete-generator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> ready to love you <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> ready to love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "configured-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s got a <unk> , he s a monster <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rocky-liverpool",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> together , yeah , yeah , yeah <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> together\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "brief-peeing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> break me off , show me what you got <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> break\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-collector",
   "metadata": {},
   "source": [
    "## 6.결과\n",
    "- test loss : 2.0839\n",
    "\n",
    "자연어처리를 통해 모델을 만드는 것을 처음 접해봐서 그런지 코드해석부터 미니프로젝트에서 어떻게 고쳐나가야할지 막막했다. 스스로 정리를 하면서 공부를 했지만 아직도 너무 어렵게 느껴진다. \n",
    "하지만 인공지능 학습모델이 단어나 문장을 입력해줬을 때 문장을 만들어내는 것이 흥미로웠다. 전에 기자라는 직업이 미래에 없어질 직업이라는 기사를 봤었는데, 이번 노드를 진행하면서 정말 이런 일이 머지 않았다는 걸 몸소 느끼기도 했다. 한편 가짜 뉴스를 만들어내는 것을 보면 그것이 가져올 일들이 두렵기도 했다. 가짜뉴스 등의 문제에 앞으로 인공지능이 어떻게 대처해나가야할지 대안이 필요하다고 생각한다. 어쩌면 훈련을 제대로 받지 않아 가짜 뉴스를 퍼트리는 인공지능에 맞서는 새로운 인공지능을 만들어내는 것도 필요할 것 같다는 생각을 하게 되는 시간이었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-young",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
